{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#from sklearn.mixture import GMM\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "#Skip to reading Step 2. way down \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"merged_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115728"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[dataset['order_status'] == 'delivered']\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',\n",
       "       'order_approved_at', 'order_delivered_carrier_date',\n",
       "       'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
       "       'quantity', 'product_id', 'seller_id', 'shipping_limit_date',\n",
       "       'product_price', 'freight_value', 'payment_sequential', 'payment_type',\n",
       "       'payment_installments', 'payment_value', 'product_name_lenght',\n",
       "       'product_description_lenght', 'product_photos_qty', 'product_weight_g',\n",
       "       'product_length_cm', 'product_height_cm', 'product_width_cm',\n",
       "       'product_category', 'review_id', 'review_score', 'review_comment_title',\n",
       "       'review_comment_message', 'review_creation_date',\n",
       "       'review_answer_timestamp', 'seller_zip_code_prefix', 'seller_city',\n",
       "       'seller_state', 'seller_lat', 'sellet_lng', 'customer_unique_id',\n",
       "       'customer_zip_code_prefix', 'customer_city', 'customer_state',\n",
       "       'customer_lat', 'customer_lng'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dropna(subset=[ 'order_delivered_customer_date', 'order_delivered_carrier_date', 'order_approved_at', 'seller_lat', 'customer_lat', 'product_category'],inplace=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>product_id</th>\n",
       "      <th>...</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>seller_lat</th>\n",
       "      <th>sellet_lng</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_lat</th>\n",
       "      <th>customer_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>3ce436f183e68e07877b285a838db11a</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-09-13T08:59:02Z</td>\n",
       "      <td>2017-09-13T09:45:35Z</td>\n",
       "      <td>2017-09-19T18:34:16Z</td>\n",
       "      <td>2017-09-20T23:43:48Z</td>\n",
       "      <td>2017-09-29T00:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>...</td>\n",
       "      <td>volta redonda</td>\n",
       "      <td>SP</td>\n",
       "      <td>-22.498183</td>\n",
       "      <td>-44.123614</td>\n",
       "      <td>871766c5855e863f6eccc05f988b23cb</td>\n",
       "      <td>28013</td>\n",
       "      <td>campos dos goytacazes</td>\n",
       "      <td>RJ</td>\n",
       "      <td>-21.758076</td>\n",
       "      <td>-41.312633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>f6dd3ec061db4e3987629fe6b26e5cce</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-04-26T10:53:06Z</td>\n",
       "      <td>2017-04-26T11:05:13Z</td>\n",
       "      <td>2017-05-04T14:35:00Z</td>\n",
       "      <td>2017-05-12T16:04:24Z</td>\n",
       "      <td>2017-05-15T00:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>...</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.566258</td>\n",
       "      <td>-46.518417</td>\n",
       "      <td>eb28e67c4c0b83846050ddfb8a35d051</td>\n",
       "      <td>15775</td>\n",
       "      <td>santa fe do sul</td>\n",
       "      <td>SP</td>\n",
       "      <td>-20.212393</td>\n",
       "      <td>-50.941471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>6489ae5e4333f3693df5ad4372dab6d3</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-01-14T14:33:31Z</td>\n",
       "      <td>2018-01-14T14:48:30Z</td>\n",
       "      <td>2018-01-16T12:36:48Z</td>\n",
       "      <td>2018-01-22T13:19:16Z</td>\n",
       "      <td>2018-02-05T00:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>...</td>\n",
       "      <td>borda da mata</td>\n",
       "      <td>MG</td>\n",
       "      <td>-22.264094</td>\n",
       "      <td>-46.158564</td>\n",
       "      <td>3818d81c6709e39d06b2738a8d3a2474</td>\n",
       "      <td>35661</td>\n",
       "      <td>para de minas</td>\n",
       "      <td>MG</td>\n",
       "      <td>-19.860439</td>\n",
       "      <td>-44.597972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>d4eb9395c8c0431ee92fce09860c5a06</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08T10:00:35Z</td>\n",
       "      <td>2018-08-08T10:10:18Z</td>\n",
       "      <td>2018-08-10T13:28:00Z</td>\n",
       "      <td>2018-08-14T13:32:39Z</td>\n",
       "      <td>2018-08-20T00:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>...</td>\n",
       "      <td>franca</td>\n",
       "      <td>SP</td>\n",
       "      <td>-20.548228</td>\n",
       "      <td>-47.395897</td>\n",
       "      <td>af861d436cfc08b2c2ddefd0ba074622</td>\n",
       "      <td>12952</td>\n",
       "      <td>atibaia</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.144923</td>\n",
       "      <td>-46.539830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>58dbd0b2d70206bf40e62cd34e84d795</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-02-04T13:57:51Z</td>\n",
       "      <td>2017-02-04T14:10:13Z</td>\n",
       "      <td>2017-02-16T09:46:09Z</td>\n",
       "      <td>2017-03-01T16:42:31Z</td>\n",
       "      <td>2017-03-17T00:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>...</td>\n",
       "      <td>loanda</td>\n",
       "      <td>PR</td>\n",
       "      <td>-22.931427</td>\n",
       "      <td>-53.133759</td>\n",
       "      <td>64b576fb70d441e8f1b2d7d446e483c5</td>\n",
       "      <td>13226</td>\n",
       "      <td>varzea paulista</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.249008</td>\n",
       "      <td>-46.824961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214  3ce436f183e68e07877b285a838db11a   \n",
       "1  00018f77f2f0320c557190d7a144bdd3  f6dd3ec061db4e3987629fe6b26e5cce   \n",
       "2  000229ec398224ef6ca0657da4fc703e  6489ae5e4333f3693df5ad4372dab6d3   \n",
       "3  00024acbcdf0a6daa1e931b038114c75  d4eb9395c8c0431ee92fce09860c5a06   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9  58dbd0b2d70206bf40e62cd34e84d795   \n",
       "\n",
       "  order_status order_purchase_timestamp     order_approved_at  \\\n",
       "0    delivered     2017-09-13T08:59:02Z  2017-09-13T09:45:35Z   \n",
       "1    delivered     2017-04-26T10:53:06Z  2017-04-26T11:05:13Z   \n",
       "2    delivered     2018-01-14T14:33:31Z  2018-01-14T14:48:30Z   \n",
       "3    delivered     2018-08-08T10:00:35Z  2018-08-08T10:10:18Z   \n",
       "4    delivered     2017-02-04T13:57:51Z  2017-02-04T14:10:13Z   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0         2017-09-19T18:34:16Z          2017-09-20T23:43:48Z   \n",
       "1         2017-05-04T14:35:00Z          2017-05-12T16:04:24Z   \n",
       "2         2018-01-16T12:36:48Z          2018-01-22T13:19:16Z   \n",
       "3         2018-08-10T13:28:00Z          2018-08-14T13:32:39Z   \n",
       "4         2017-02-16T09:46:09Z          2017-03-01T16:42:31Z   \n",
       "\n",
       "  order_estimated_delivery_date  quantity                        product_id  \\\n",
       "0          2017-09-29T00:00:00Z         1  4244733e06e7ecb4970a6e2683c13e61   \n",
       "1          2017-05-15T00:00:00Z         1  e5f2d52b802189ee658865ca93d83a8f   \n",
       "2          2018-02-05T00:00:00Z         1  c777355d18b72b67abbeef9df44fd0fd   \n",
       "3          2018-08-20T00:00:00Z         1  7634da152a4610f1595efa32f14722fc   \n",
       "4          2017-03-17T00:00:00Z         1  ac6c3623068f30de03045865e4e10089   \n",
       "\n",
       "   ...    seller_city seller_state  seller_lat  sellet_lng  \\\n",
       "0  ...  volta redonda           SP  -22.498183  -44.123614   \n",
       "1  ...      sao paulo           SP  -23.566258  -46.518417   \n",
       "2  ...  borda da mata           MG  -22.264094  -46.158564   \n",
       "3  ...         franca           SP  -20.548228  -47.395897   \n",
       "4  ...         loanda           PR  -22.931427  -53.133759   \n",
       "\n",
       "                 customer_unique_id customer_zip_code_prefix  \\\n",
       "0  871766c5855e863f6eccc05f988b23cb                    28013   \n",
       "1  eb28e67c4c0b83846050ddfb8a35d051                    15775   \n",
       "2  3818d81c6709e39d06b2738a8d3a2474                    35661   \n",
       "3  af861d436cfc08b2c2ddefd0ba074622                    12952   \n",
       "4  64b576fb70d441e8f1b2d7d446e483c5                    13226   \n",
       "\n",
       "           customer_city  customer_state  customer_lat  customer_lng  \n",
       "0  campos dos goytacazes              RJ    -21.758076    -41.312633  \n",
       "1        santa fe do sul              SP    -20.212393    -50.941471  \n",
       "2          para de minas              MG    -19.860439    -44.597972  \n",
       "3                atibaia              SP    -23.144923    -46.539830  \n",
       "4        varzea paulista              SP    -23.249008    -46.824961  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.order_purchase_timestamp = pd.to_datetime(dataset.order_purchase_timestamp)\n",
    "#dataset.order_aproved_at = pd.to_datetime(dataset.order_aproved_at)\n",
    "dataset.order_estimated_delivery_date = pd.to_datetime(dataset.order_estimated_delivery_date)\n",
    "dataset.order_delivered_customer_date = pd.to_datetime(dataset.order_delivered_customer_date)\n",
    "dataset.order_approved_at = pd.to_datetime(dataset.order_approved_at)\n",
    "dataset.shipping_limit_date = pd.to_datetime(dataset.shipping_limit_date)\n",
    "dataset.order_delivered_carrier_date = pd.to_datetime(dataset.order_delivered_carrier_date)\n",
    "dataset.review_creation_date = pd.to_datetime(dataset.review_creation_date)\n",
    "dataset.review_answer_timestamp = pd.to_datetime(dataset.review_answer_timestamp)\n",
    "dataset.product_category.replace(['None', 'NaN', np.nan], \"NIL\", inplace=True)\n",
    "dataset['freight_rate'] = dataset[['freight_value','quantity']].apply(\n",
    "    lambda row : round(row['freight_value'] / row['quantity'],2), axis=1\n",
    ")\n",
    "\n",
    "#dataset.pivot(index='', columns='name', values='dollars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_unique_id','review_id', 'quantity', 'payment_type']\n",
    "#dataset = dataset.drop(\"index\", axis = 1)\n",
    "dataset['payment_sum'] = dataset.groupby(['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_unique_id','review_id', 'quantity', 'payment_type'])['payment_value'].transform('sum')\n",
    "dataset.drop_duplicates(subset = columns, keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.pivot_table(dataset, index=['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_unique_id','review_id', 'quantity']\n",
    "               , columns='payment_type', values='payment_sum', fill_value = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove(\"payment_type\")\n",
    "dataset.drop_duplicates(subset = columns, keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(dataset,dataset2, on = ['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_unique_id','review_id', 'quantity'], how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['payment_type', 'payment_sum'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 108716 entries, 0 to 108715\n",
      "Data columns (total 47 columns):\n",
      "order_id                         108716 non-null object\n",
      "customer_id                      108716 non-null object\n",
      "order_status                     108716 non-null object\n",
      "order_purchase_timestamp         108716 non-null datetime64[ns, UTC]\n",
      "order_approved_at                108716 non-null datetime64[ns, UTC]\n",
      "order_delivered_carrier_date     108716 non-null datetime64[ns, UTC]\n",
      "order_delivered_customer_date    108716 non-null datetime64[ns, UTC]\n",
      "order_estimated_delivery_date    108716 non-null datetime64[ns, UTC]\n",
      "quantity                         108716 non-null int64\n",
      "product_id                       108716 non-null object\n",
      "seller_id                        108716 non-null object\n",
      "shipping_limit_date              108716 non-null datetime64[ns, UTC]\n",
      "product_price                    108716 non-null float64\n",
      "freight_value                    108716 non-null float64\n",
      "payment_sequential               108716 non-null int64\n",
      "payment_installments             108716 non-null int64\n",
      "payment_value                    108716 non-null float64\n",
      "product_name_lenght              108716 non-null float64\n",
      "product_description_lenght       108716 non-null float64\n",
      "product_photos_qty               108716 non-null float64\n",
      "product_weight_g                 108715 non-null float64\n",
      "product_length_cm                108715 non-null float64\n",
      "product_height_cm                108715 non-null float64\n",
      "product_width_cm                 108715 non-null float64\n",
      "product_category                 108716 non-null object\n",
      "review_id                        108716 non-null object\n",
      "review_score                     108716 non-null int64\n",
      "review_comment_title             13133 non-null object\n",
      "review_comment_message           46040 non-null object\n",
      "review_creation_date             108716 non-null datetime64[ns, UTC]\n",
      "review_answer_timestamp          108716 non-null datetime64[ns, UTC]\n",
      "seller_zip_code_prefix           108716 non-null int64\n",
      "seller_city                      108716 non-null object\n",
      "seller_state                     108716 non-null object\n",
      "seller_lat                       108716 non-null float64\n",
      "sellet_lng                       108716 non-null float64\n",
      "customer_unique_id               108716 non-null object\n",
      "customer_zip_code_prefix         108716 non-null int64\n",
      "customer_city                    108716 non-null object\n",
      "customer_state                   108716 non-null object\n",
      "customer_lat                     108716 non-null float64\n",
      "customer_lng                     108716 non-null float64\n",
      "freight_rate                     108716 non-null float64\n",
      "boleto                           108716 non-null float64\n",
      "credit_card                      108716 non-null float64\n",
      "debit_card                       108716 non-null float64\n",
      "voucher                          108716 non-null float64\n",
      "dtypes: datetime64[ns, UTC](8), float64(19), int64(6), object(14)\n",
      "memory usage: 39.8+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(1234)    \n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86973 21743\n"
     ]
    }
   ],
   "source": [
    "train, test = split_train_test(dataset, 0.2)\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['review_score'] >= 3 \n",
    "y_test = test['review_score'] >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y, y_pred):\n",
    "    accuracy = sum(y_pred == y)/len(y)\n",
    "    c_m = confusion_matrix(y, y_pred)\n",
    "    TP = c_m[1][1]\n",
    "    FP = c_m[0][1]\n",
    "    FN = c_m[1][0]\n",
    "    TN = c_m[0][0]\n",
    "    #print(confusion_matrix(y, y_pred))\n",
    "    BER = 1 - 0.5 * (TP / (TP + FN) + TN / (TN + FP))\n",
    "    return accuracy, BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hei_prod(train):\n",
    "    product_cat = train.groupby('product_category').order_purchase_timestamp.count().reset_index()\n",
    "    product_cat.columns = ['product_category', 'Frequency']\n",
    "    ms = train.groupby('product_category').payment_value.sum().reset_index()\n",
    "    ms.columns = ['product_category','Monetary']\n",
    "    df_prd = pd.merge(product_cat,ms, on='product_category')\n",
    "    pd_f_log = np.log(df_prd ['Frequency'])\n",
    "    pd_m_log = np.log(df_prd ['Monetary']+0.1)\n",
    "    log_pd_data = pd.DataFrame({'Monetary': pd_m_log,'Frequency': pd_f_log})\n",
    "    linked = linkage(log_pd_data, 'weighted')\n",
    "    k=2\n",
    "    l1=pd.DataFrame(fcluster(linked, k, criterion='maxclust'))\n",
    "    k=3\n",
    "    l2=pd.DataFrame(fcluster(linked, k, criterion='maxclust'))\n",
    "    k=5\n",
    "    level3=fcluster(linked, k, criterion='maxclust')\n",
    "    k=7\n",
    "    l3= pd.DataFrame(fcluster(linked, k, criterion='maxclust'))\n",
    "    k=10\n",
    "    l4= pd.DataFrame(fcluster(linked, k, criterion='maxclust'))\n",
    "    TH = pd.DataFrame({'productcategory': list(df_prd['product_category']),'Level1': l1[0],'Level2': l2[0],'Level3': l3[0],'Level4': l4[0] })\n",
    "    level_onehot = TH.copy()\n",
    "    level_onehot = pd.get_dummies(level_onehot, columns=['Level1','Level2','Level3','Level4'])\n",
    "    return level_onehot\n",
    "    \n",
    "def kclus_cust(train):\n",
    "    recency_scores = train.groupby('customer_unique_id').order_purchase_timestamp.max().reset_index()\n",
    "    recency_scores.columns = ['customer_unique_id', 'MaxPurchaseDate']\n",
    "    recency_scores['Recency'] = (recency_scores['MaxPurchaseDate'].max() - recency_scores['MaxPurchaseDate']).dt.days\n",
    "    frequency_scores = train.groupby('customer_unique_id').order_purchase_timestamp.count().reset_index()\n",
    "    frequency_scores.columns = ['customer_unique_id','Frequency']\n",
    "    monetory_scores = train.groupby('customer_unique_id').payment_value.sum().reset_index()\n",
    "    monetory_scores.columns = ['customer_unique_id','Monetary']\n",
    "    df_rfm = pd.merge(recency_scores, frequency_scores, on='customer_unique_id')\n",
    "    df_rfm = pd.merge(df_rfm, monetory_scores, on='customer_unique_id')\n",
    "    rfm_r_log = np.log(df_rfm['Recency']+0.1) #can't take log(0) and so add a small number\n",
    "    rfm_f_log = np.log(df_rfm['Frequency'])\n",
    "    rfm_m_log = np.log(df_rfm['Monetary']+0.1)\n",
    "    log_data = pd.DataFrame({'Monetary': rfm_m_log,'Recency': rfm_r_log,'Frequency': rfm_f_log})\n",
    "    matrix = log_data.as_matrix()\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = 2, n_init=100)\n",
    "    kmeans.fit(matrix)\n",
    "    clusters = kmeans.predict(matrix)\n",
    "    clusters1= pd.DataFrame({'Customer':df_rfm['customer_unique_id'], 'cluster': clusters})\n",
    "    return(clusters1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_product = hei_prod(dataset)\n",
    "\n",
    "#one_hot_customer = kclus_cust(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c780eb057b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     photos, price, distance, time_to_review.days, popularity, freight_rate]\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-c780eb057b1f>\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfreight_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"freight_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     return feat1 + [review_length, delivery_time.days, approving_time.days, approving_time.days, processing_time.days, \n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mshipping_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurchase_month\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredit_pay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebit_pay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoucher_pay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboleto_wallet_pay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     photos, price, distance, time_to_review.days, popularity, freight_rate]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat1' is not defined"
     ]
    }
   ],
   "source": [
    "categories = dataset['product_category'].unique().tolist()\n",
    "seller_popularities = pd.DataFrame(train.groupby('seller_id').size().reset_index(name = \"Count\"))\n",
    "\n",
    "\n",
    "def features(data):\n",
    "    data.fillna(0)\n",
    "    \n",
    "    #feat1 = one_hot_product[one_hot_product['productcategory'] == data['product_category']].iloc[0, 1:].tolist()\n",
    "    #feat2 = one_hot_customer[one_hot_customer['Customer'] == data['customer_unique_id']].cluster.tolist()\n",
    "    \n",
    "    review_length = 0 if (pd.isnull(data['review_comment_message'])) else len(data['review_comment_message'])\n",
    "    delivery_time = data['order_delivered_customer_date'] - data['order_estimated_delivery_date']\n",
    "    approving_time = data['order_approved_at'] - data['order_purchase_timestamp']\n",
    "    processing_time = data['order_delivered_carrier_date'] - data['order_approved_at'] \n",
    "    shipping_time = data['shipping_limit_date'] - data['order_delivered_carrier_date']\n",
    "    purchase_month = data['order_purchase_timestamp'].month\n",
    "    credit_pay = data['credit_card']\n",
    "    debit_pay = data['debit_card']\n",
    "    voucher_pay = data['voucher']\n",
    "    boleto_wallet_pay = data['boleto']\n",
    "    price = 0 if (pd.isnull(data['product_price'])) else data['product_price']\n",
    "    \n",
    "    desc_length = 0 if (pd.isnull(data['product_description_lenght'])) else data['product_description_lenght']\n",
    "    photos = 0 if (pd.isnull(data['product_photos_qty'])) else data['product_photos_qty']\n",
    "    price = data['product_price']\n",
    "    time_to_review = data['review_answer_timestamp'] - data['order_delivered_customer_date']\n",
    "    seller_coord = (data['seller_lat'], data['sellet_lng'])\n",
    "    customer_coord = (data['customer_lat'], data['customer_lng'])\n",
    "    distance = geodesic(seller_coord, customer_coord).miles\n",
    "    popularity = seller_popularities[seller_popularities[\"seller_id\"] == data[\"seller_id\"]].Count.values[0] if data[\"seller_id\"] in seller_popularities.seller_id else 0\n",
    "    freight_rate = data[\"freight_rate\"]\n",
    "    \n",
    "    return [review_length, delivery_time.days, approving_time.days, approving_time.days, processing_time.days, \n",
    "                    shipping_time.days, purchase_month, credit_pay, debit_pay, voucher_pay, boleto_wallet_pay, desc_length,\n",
    "                    photos, price, distance, time_to_review.days, popularity, freight_rate]\n",
    "\n",
    "features(dataset.iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [features(row) for _, row in train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [features(row) for _, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "std_X_train = min_max_scaler.fit_transform(X_train)\n",
    "std_X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classification models\n",
    "\n",
    "def perform_regression(val, X_train, y_train, X_test, y_test, hyperparameters):\n",
    "    acc_train = []\n",
    "    BER_valid = []\n",
    "    acc_test = []\n",
    "    for c in hyperparameters:\n",
    "        if val == 1:\n",
    "            model = LogisticRegression(C = c)\n",
    "        if val == 2:\n",
    "            model = LinearSVC(C=c)\n",
    "        tr_acc = cross_val_score(model, X_train, y_train, scoring='accuracy', cv = 3).mean() * 100\n",
    "        acc_train.append(tr_acc)\n",
    "    plt.title('Accuracy vs Hyperparameter value')\n",
    "    plt.plot(hyperparameters, acc_train, \"b-+\", label=\"train\")\n",
    "    #plt.plot(hyperparameters, BER_valid, \"g-*\", label=\"valid\")\n",
    "    #plt.plot(hyperparameters, acc_test, \"r-*\", label=\"test\")\n",
    "    plt.xlabel(\"Hyperparameter value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "hyperparameters = [ 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_regression(1, std_X_train, y_train, std_X_test, y_test, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C = 10 )\n",
    "log_reg.fit(std_X_train, y_train)\n",
    "\n",
    "y_train_pred = log_reg.predict(std_X_train)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "      \n",
    "y_test_pred = log_reg.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_regression(2, std_X_train, y_train, std_X_test, y_test, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_SVM = LinearSVC(C=10)\n",
    "lin_SVM.fit(std_X_train, y_train)\n",
    "\n",
    "y_train_pred = lin_SVM.predict(std_X_train)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "      \n",
    "      \n",
    "y_test_pred = lin_SVM.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C = 1.0, class_weight='balanced')\n",
    "random_forest = RandomForestClassifier(n_estimators=5)\n",
    "lin_SVM = SVC(kernel=\"linear\", C=1.0)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(random_forest, std_X_train, y_train, scoring='accuracy', cv = 5).mean() * 100\n",
    "print(\"Accuracy of Random Forest is: \" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'n_estimators': [1, 10, 20, 100, 200], 'max_features': [2, 3]}\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, param_grid = grid_values, scoring = 'accuracy', cv = 3, return_train_score=True)\n",
    "\n",
    "grid_search.fit(std_X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized Search for random forest model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [1, 10, 20, 100, 200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [80, 90, 100]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "#rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "#rf_random.fit(std_X_train, y_train)\n",
    "\n",
    "#rf_random.best_params_\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(std_X_train,y_train)\n",
    "y_train_pred = random_forest.predict(std_X_train)\n",
    "performance_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(max_features = 3, n_estimators = 200, max_depth = 50)\n",
    "\n",
    "random_forest.fit(std_X_train,y_train)\n",
    "\n",
    "\n",
    "feature_importances = pd.DataFrame(random_forest.feature_importances_,\n",
    "                                   index = range(1, 41),\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(max_features = 2, n_estimators = 200, max_depth = 40)\n",
    "\n",
    "random_forest.fit(std_X_train2,y_train)\n",
    "\n",
    "y_train_pred = random_forest.predict(std_X_train2)\n",
    "performance_metrics(y_train, y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = random_forest.predict(std_X_train)\n",
    "performance_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = random_forest.predict(std_X_test2)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [20, 100, 200]\n",
    "max_features = [2, 3, 5]\n",
    "max_depth = [20, 40, 50]\n",
    "\n",
    "thresholds_criteria = list(itertools.product(n_estimators, max_features, max_depth))\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "n_est = [] \n",
    "max_f = []\n",
    "max_d = []\n",
    "\n",
    "for n, m, d in thresholds_criteria:\n",
    "    rf = RandomForestClassifier(max_features = m, n_estimators = n, max_depth = d)\n",
    "    rf.fit(std_X_train2,y_train)\n",
    "    y_train_pred = rf.predict(std_X_train2)\n",
    "    acc, _ = performance_metrics(y_train, y_train_pred)\n",
    "    train_accuracy.append(acc)\n",
    "    y_test_pred = rf.predict(std_X_test2)\n",
    "    acc, _ = performance_metrics(y_test, y_test_pred)\n",
    "    test_accuracy.append(acc)\n",
    "    n_est.append(n)\n",
    "    max_f.append(m)\n",
    "    max_d.append(d)\n",
    "    \n",
    "df = {\"N-estimators\": n_est,\n",
    "      \"Max_features\": max_f,\n",
    "      \"Max_Depth\": max_d,\n",
    "      \"Train_accuracy\": train_accuracy,\n",
    "      \"Test Accuracy\": test_accuracy}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "#printing dataframe for summary\n",
    "df.sort_values(\n",
    "    by=[\"Test Accuracy\", \"Train_accuracy\"], ascending=[False, False]\n",
    ")[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing dataframe for summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(lin_SVM, std_X_train, y_train, scoring='accuracy', cv = 5).mean() * 100\n",
    "print(\"Accuracy of Random Forest is: \" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM model tuning\n",
    "perform_regression(2, std_X_train, y_train, std_X_test, y_test, hyperparameters)\n",
    "\n",
    "lin_SVM = SVC(kernel=\"linear\", C=1.0)\n",
    "lin_SVM.fit(std_X_train, y_train)\n",
    "\n",
    "y_train_pred = lin_SVM.predict(std_X_train)\n",
    "performance_metrics(y_train, y_train_pred)\n",
    "\n",
    "y_test_pred = lin_SVM.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(knn_classifier, std_X_train, y_train, scoring='accuracy', cv = 5).mean() * 100\n",
    "print(\"Accuracy of KNN is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier.fit(std_X_train, y_train)\n",
    "knn_classifier.score(std_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = random_forest.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(y_test, [1] * len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(data):\n",
    "    data.fillna(0)\n",
    "    \n",
    "    feat1 = one_hot_product[one_hot_product['productcategory'] == data['product_category']].iloc[0, 1:].tolist()\n",
    "    #feat2 = one_hot_customer[one_hot_customer['Customer'] == data['customer_unique_id']].cluster.tolist()\n",
    "    \n",
    "    review_length = 0 if (pd.isnull(data['review_comment_message'])) else len(data['review_comment_message'])\n",
    "    delivery_time = data['order_delivered_customer_date'] - data['order_estimated_delivery_date']\n",
    "    approving_time = data['order_approved_at'] - data['order_purchase_timestamp']\n",
    "    processing_time = data['order_delivered_carrier_date'] - data['order_approved_at'] \n",
    "    shipping_time = data['shipping_limit_date'] - data['order_delivered_carrier_date']\n",
    "    purchase_month = data['order_purchase_timestamp'].month\n",
    "    credit_pay = data['credit_card']\n",
    "    debit_pay = data['debit_card']\n",
    "    voucher_pay = data['voucher']\n",
    "    boleto_wallet_pay = data['boleto']\n",
    "    price = 0 if (pd.isnull(data['product_price'])) else data['product_price']\n",
    "    \n",
    "    desc_length = 0 if (pd.isnull(data['product_description_lenght'])) else data['product_description_lenght']\n",
    "    photos = 0 if (pd.isnull(data['product_photos_qty'])) else data['product_photos_qty']\n",
    "    price = data['product_price']\n",
    "    time_to_review = data['review_answer_timestamp'] - data['order_delivered_customer_date']\n",
    "    seller_coord = (data['seller_lat'], data['sellet_lng'])\n",
    "    customer_coord = (data['customer_lat'], data['customer_lng'])\n",
    "    distance = geodesic(seller_coord, customer_coord).miles\n",
    "    popularity = seller_popularities[seller_popularities[\"seller_id\"] == data[\"seller_id\"]].Count.values[0] if data[\"seller_id\"] in seller_popularities.seller_id else 0\n",
    "    freight_rate = data[\"freight_rate\"]\n",
    "    \n",
    "    return [review_length, delivery_time.days, approving_time.days, approving_time.days, processing_time.days, \n",
    "                    shipping_time.days, purchase_month, credit_pay, debit_pay, voucher_pay, boleto_wallet_pay, desc_length,\n",
    "                    photos, price, distance, time_to_review.days, popularity, freight_rate]\n",
    "\n",
    "features(dataset.iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = [features(row) for _, row in train.iterrows()]\n",
    "X_test2 = [features(row) for _, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "std_X_train2 = min_max_scaler.fit_transform(X_train2)\n",
    "std_X_test2 = min_max_scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classification models\n",
    "\n",
    "def perform_regression(val, X_train, y_train, X_test, y_test, hyperparameters):\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    for c in hyperparameters:\n",
    "        \n",
    "        model = LogisticRegression(C = c)\n",
    "        if val == 2:\n",
    "            model = LinearSVC(C=c)\n",
    "        if val == 3:\n",
    "            model = knn_classifier = KNeighborsClassifier(n_neighbors=c)\n",
    "        valid_acc = cross_val_score(model, X_train, y_train, scoring='accuracy', cv = 3).mean() * 100\n",
    "        acc_train.append(valid_acc)\n",
    "        #odel.fit(X_train, y_train)\n",
    "        #acc,_ = performance_metrics(y_test, model.predict(X_test))\n",
    "        #cc_test.append(acc)\n",
    "    plt.title('Accuracy vs Hyperparameter value')\n",
    "    plt.plot(hyperparameters, acc_train, \"b-+\", label=\"train\")\n",
    "    #plt.plot(hyperparameters, acc_test, \"r-*\", label=\"test\")\n",
    "    plt.xlabel(\"Hyperparameter value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "hyperparameters = [ 0.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_regression(1, std_X_train3, y_train, std_X_test3, y_test, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C = 100)\n",
    "log_reg.fit(std_X_train3, y_train)\n",
    "\n",
    "y_train_pred = log_reg.predict(std_X_train3)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "      \n",
    "      \n",
    "y_test_pred = log_reg.predict(std_X_test3)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_regression(2, std_X_train2, y_train, std_X_test2, y_test, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_SVM = LinearSVC(C=100)\n",
    "lin_SVM.fit(std_X_train2, y_train)\n",
    "\n",
    "y_train_pred = lin_SVM.predict(std_X_train2)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "      \n",
    "      \n",
    "y_test_pred = lin_SVM.predict(std_X_test2)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [2, 3, 5, 7, 10]\n",
    "perform_regression(3, std_X_train, y_train, std_X_test, y_test, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(std_X_train, y_train)\n",
    "\n",
    "y_train_pred = knn_classifier.predict(std_X_train)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = knn_classifier.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def features(data):\n",
    "    data.fillna(0)\n",
    "    \n",
    "    feat1 = one_hot_product[one_hot_product['productcategory'] == data['product_category']].iloc[0, 1:].tolist()\n",
    "    #feat2 = one_hot_customer[one_hot_customer['Customer'] == data['customer_unique_id']].cluster.tolist()\n",
    "    \n",
    "    review_length = 0 if (pd.isnull(data['review_comment_message'])) else len(data['review_comment_message'])\n",
    "    delivery_time = data['order_delivered_customer_date'] - data['order_estimated_delivery_date']\n",
    "    approving_time = data['order_approved_at'] - data['order_purchase_timestamp']\n",
    "    processing_time = data['order_delivered_carrier_date'] - data['order_approved_at'] \n",
    "    shipping_time = data['shipping_limit_date'] - data['order_delivered_carrier_date']\n",
    "    purchase_month = data['order_purchase_timestamp'].month\n",
    "    credit_pay = data['credit_card']\n",
    "    debit_pay = data['debit_card']\n",
    "    voucher_pay = data['voucher']\n",
    "    boleto_wallet_pay = data['boleto']\n",
    "    price = 0 if (pd.isnull(data['product_price'])) else data['product_price']\n",
    "    \n",
    "    desc_length = 0 if (pd.isnull(data['product_description_lenght'])) else data['product_description_lenght']\n",
    "    photos = 0 if (pd.isnull(data['product_photos_qty'])) else data['product_photos_qty']\n",
    "    price = data['product_price']\n",
    "    time_to_review = data['review_answer_timestamp'] - data['order_delivered_customer_date']\n",
    "    seller_coord = (data['seller_lat'], data['sellet_lng'])\n",
    "    customer_coord = (data['customer_lat'], data['customer_lng'])\n",
    "    distance = geodesic(seller_coord, customer_coord).miles\n",
    "    popularity = seller_popularities[seller_popularities[\"seller_id\"] == data[\"seller_id\"]].Count.values[0] if data[\"seller_id\"] in seller_popularities.seller_id else 0\n",
    "    freight_rate = data[\"freight_rate\"]\n",
    "    \n",
    "    return [review_length, desc_length,\n",
    "                    photos, price, popularity, freight_rate]\n",
    "\n",
    "features(dataset.iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = [features(row) for _, row in train.iterrows()]\n",
    "X_test3 = [features(row) for _, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "std_X_train3 = min_max_scaler.fit_transform(X_train3)\n",
    "std_X_test3 = min_max_scaler.transform(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dde7bd5d350a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrbf_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rbf_svc = SVC(kernel='rbf', gamma=0.7, C=100)\n",
    "\n",
    "rbf_svc.fit(std_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rbf_svc.predict(std_X_train)\n",
    "\n",
    "print(performance_metrics(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = rbf_svc.predict(std_X_test)\n",
    "performance_metrics(y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
